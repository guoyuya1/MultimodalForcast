{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Adjust the path as needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time\n",
    "from einops import rearrange\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from MultimodalForcast.data_loader.data_loader import process_bitcoin_data, split_series, TimeSeriesDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 91 rows in the filtered bitcoin dataframe\n",
      "Batch shapes:\n",
      "Texts: 8 sequences, each with 10 articles\n",
      "Values shape: torch.Size([8, 10, 1])\n",
      "Targets shape: torch.Size([8, 10, 1])\n",
      "\n",
      "First sequence:\n",
      "Values: tensor([[8605.],\n",
      "        [8778.],\n",
      "        [8693.],\n",
      "        [8869.],\n",
      "        [8907.],\n",
      "        [8859.],\n",
      "        [8670.],\n",
      "        [8672.],\n",
      "        [8693.],\n",
      "        [8469.]])\n",
      "Targets: tensor([[8410.],\n",
      "        [8349.],\n",
      "        [8466.],\n",
      "        [8744.],\n",
      "        [9049.],\n",
      "        [9349.],\n",
      "        [9394.],\n",
      "        [9366.],\n",
      "        [9393.],\n",
      "        [9398.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyan/MultimodalForcast/model/../../MultimodalForcast/data_loader/data_loader.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return x_text, torch.tensor(x_value, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2020-03-31'\n",
    "ts_file_path = '../data/bitcoin/bitcoin_daily.csv'\n",
    "news_file_path = '../data/bitcoin/bitcoin_news.json'\n",
    "lookback = 10\n",
    "predict = 10\n",
    "\n",
    "df_filtered = process_bitcoin_data(ts_file_path, news_file_path, start_date, end_date)\n",
    "train_data, val_data, test_data = split_series(df_filtered, lookback)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, lookback, predict)\n",
    "val_dataset = TimeSeriesDataset(val_data, lookback, predict)\n",
    "test_dataset = TimeSeriesDataset(test_data, lookback, predict)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "texts = batch['text']      # List of article sequences\n",
    "values = batch['value']    # Tensor of value sequences\n",
    "targets = batch['target']  # Tensor of target values\n",
    "\n",
    "# Print shapes and sample content\n",
    "print(\"Batch shapes:\")\n",
    "print(f\"Texts: {len(texts)} sequences, each with {len(texts[0])} articles\")\n",
    "print(f\"Values shape: {values.shape}\")  # Should be [batch_size, lookback_window]\n",
    "print(f\"Targets shape: {targets.shape}\")  # Should be [batch_size, prediction_window]\n",
    "\n",
    "# Print first sequence's content\n",
    "print(\"\\nFirst sequence:\")\n",
    "# print(\"Texts:\", texts[0])\n",
    "print(\"Values:\", values[0])\n",
    "print(\"Targets:\", targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  draft of pretrained bert text encoder\n",
    "# # pretrained bert text encoder - draft\n",
    "# # bert-base-uncased: 768\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# # tocuda\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# texts = df_filtered['full_article'].values.tolist()\n",
    "# # Removing special characters (keeping only alphanumeric and spaces)\n",
    "# #If text is not a string, then NA\n",
    "# texts = [\n",
    "#     re.sub(r'[^a-zA-Z0-9 ]+', \"\", str(text)) \n",
    "#     if pd.notna(text) and isinstance(text, (str, float, int)) \n",
    "#     else \"NA\" \n",
    "#     for text in texts\n",
    "# ]\n",
    "\n",
    "\n",
    "# batch_size = 8 # number of text sequences in one batch\n",
    "# all_embeddings = []\n",
    "\n",
    "# # Process each batch\n",
    "# for i in range(0, len(texts), batch_size):\n",
    "#     batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "#     # Tokenize the batch\n",
    "#     # padding=True, truncation=True, max_length=512\n",
    "#     # tocuda\n",
    "#     # inputs: [batch_size, sequence_length]\n",
    "#     inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "#     # Get the embeddings\n",
    "#     with torch.no_grad():\n",
    "#         # outputs: [batch_size, sequence_length, bert_output_hidden_size]\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     # [CLS]\n",
    "#     # batch_embeddings: [batch_size, bert_output_hidden_size]\n",
    "#     batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#     all_embeddings.append(batch_embeddings)\n",
    "\n",
    "# # Combine all batches    \n",
    "# if all_embeddings:\n",
    "#     # all_embeddings: [total_number_of_texts, bert_output_hidden_size]\n",
    "#     all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# print(all_embeddings.shape)\n",
    "\n",
    "# # text encoder\n",
    "# # input: list of list\n",
    "# # output: [batch size, look_back, bert_hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrained_Bert_Encoder(nn.Module):\n",
    "    def __init__(self, device, finetune=False):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        \n",
    "        self.device = device\n",
    "        self.bert = self.bert.to(device)\n",
    "        # self.linear = self.linear.to(device)\n",
    "        \n",
    "        # Freeze BERT parameters\n",
    "        if finetune == False:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        # texts is a list of strings\n",
    "        texts = [\n",
    "        re.sub(r'[^a-zA-Z0-9 ]+', \"\", str(text)) \n",
    "        if pd.notna(text) and isinstance(text, (str, float, int)) \n",
    "        else \"NA\" \n",
    "        for text in texts\n",
    "    ]\n",
    "        return texts\n",
    "            \n",
    "    def forward(self, texts):\n",
    "        # texts is a list of lists: [batch_size, lookback]\n",
    "        \n",
    "        # Flatten the list of lists into a single list\n",
    "        flat_texts = [article for sample_texts in texts for article in sample_texts]\n",
    "        \n",
    "        # Preprocess all texts\n",
    "        flat_texts = self.preprocess_text(flat_texts)\n",
    "        \n",
    "        # Tokenize and get embeddings\n",
    "        inputs = self.tokenizer(flat_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.bert.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(**inputs)\n",
    "        \n",
    "        # Get [CLS] token embeddings\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [total_articles, 768]\n",
    "\n",
    "        \n",
    "        # Reshape back to [batch_size, lookback, bert_output_dim]\n",
    "        batch_size = len(texts)\n",
    "        lookback = len(texts[0])  #lookback\n",
    "        reshaped_output = cls_embeddings.reshape(batch_size, lookback, -1)\n",
    "        \n",
    "        return reshaped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test encoder\n",
    "encoder = Pretrained_Bert_Encoder(device=\"cpu\")\n",
    "encoder.forward([[\"article1\", \"article2\"], [\"article2\", \"article3\"], [\"article3\", \"article4\"]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RevIN (Reversible Instance Normalization)\n",
    "\n",
    "class RevIn(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: Number of input features\n",
    "        :param eps: Stability add-on value\n",
    "        :param affine: If True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIn, self).__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        # x is [batch_size, seq_len, num_features]:\n",
    "        # range(1, x.ndim - 1) = range(1, 2) = (1,)\n",
    "        # along along sequence dimension\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last.to(x.device)\n",
    "        else:\n",
    "            x = x - self.mean.to(x.device)\n",
    "        x = x / self.stdev.to(x.device)\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight.to(x.device)\n",
    "            x = x + self.affine_bias.to(x.device)\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias.to(x.device)\n",
    "            x = x / (self.affine_weight.to(x.device) + self.eps * self.eps)\n",
    "        x = x * self.stdev.to(x.device)\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last.to(x.device)\n",
    "        else:\n",
    "            x = x + self.mean.to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # detach() example\n",
    "# w = nn.Parameter(torch.tensor([2.0]), requires_grad=True)\n",
    "\n",
    "# # Case 1: No detach\n",
    "# result = 2*w + w  # = 3w\n",
    "# loss = some_function(result)\n",
    "# loss.backward()\n",
    "# # Gradient flows: loss -> result -> w\n",
    "# # w.grad will include factor of 3 (both paths contribute)\n",
    "# # If loss gradient is 1, w.grad would be 3\n",
    "\n",
    "# # Case 2: Partial detach\n",
    "# result = (2*w).detach() + w  # = 2(constant) + w\n",
    "# loss = some_function(result)\n",
    "# loss.backward()\n",
    "# # Gradient flows only through the 'w' term\n",
    "# # 2*w is treated as constant\n",
    "# # If loss gradient is 1, w.grad would be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multimodal_gpt4mts(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patching parameters\n",
    "        self.patch_size = config.patch_size\n",
    "        self.stride = config.stride\n",
    "        self.patch_num = (config.lookback - self.patch_size) // self.stride + 2\n",
    "        self.revin = config.revin\n",
    "        self.device = device\n",
    "\n",
    "        # Original sequence: [1, 2, 3, 4, 5]\n",
    "        # With (0, 2) padding:\n",
    "        # - Add 0 elements at start\n",
    "        # - Add 2 elements at end by replicating last value\n",
    "        # Result: [1, 2, 3, 4, 5, 5, 5]\n",
    "\n",
    "        # If we used (2, 2):\n",
    "        # - Add 2 elements at start by replicating first value\n",
    "        # - Add 2 elements at end by replicating last value\n",
    "        # Result: [1, 1, 1, 2, 3, 4, 5, 5, 5]\n",
    "        self.padding_patch_layer = nn.ReplicationPad1d((0, config.stride))\n",
    "      \n",
    "\n",
    "        # encoder decoder\n",
    "        \n",
    "        self.in_layer = nn.Linear(config.patch_size, config.model_hidden_dim)\n",
    "        self.prompt_layer = nn.Linear(config.model_hidden_dim, config.model_hidden_dim)\n",
    "        self.out_layer = nn.Linear(config.model_hidden_dim * (self.patch_num), config.predict)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rev_in = RevIn(config.num_features)\n",
    "        \n",
    "        # text encoder, currently frozen\n",
    "        self.text_encoder = Pretrained_Bert_Encoder(finetune=False, device=device)\n",
    "\n",
    "        if config.pretraingpt2:\n",
    "            self.decoder = GPT2Model.from_pretrained('gpt2')\n",
    "        else:\n",
    "            self.decoder = GPT2Model(GPT2Config())\n",
    "\n",
    "\n",
    "        if config.pretraingpt2 and config.finetunedecoder:\n",
    "            # Only fine-tunes layer normalization and positional embeddings\n",
    "            # Layer norms help adapt to new data distributions\n",
    "            # layer norm operates on the last dimension - 766 for each patch\n",
    "\n",
    "            for i, (name, param) in enumerate(self.decoder.named_parameters()):\n",
    "                if 'ln' in name or 'wpe' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "\n",
    "        for layer in (self.decoder, self.in_layer, self.out_layer, self.prompt_layer):\n",
    "            layer.to(device=device)\n",
    "\n",
    "    def get_patch_text_embeddings(self, texts):\n",
    "        text_embeddings = self.text_encoder(texts)\n",
    "        text_embeddings = rearrange(text_embeddings, 'b l m -> b m l') # [batch_size, 768, lookback]\n",
    "        text_embeddings = self.padding_patch_layer(text_embeddings) # [batch_size, 768, lookback + stride]\n",
    "\n",
    "        # patch along the last dimension\n",
    "        text_embeddings = text_embeddings.unfold(dimension=-1, size=self.patch_size, step=self.stride) #[batch_size, 768, num_patches, patch_size]\n",
    "        # average text embedding of each patch\n",
    "        text_embeddings = text_embeddings.mean(dim=-1).squeeze() # [batch_size, 768, num_patches]\n",
    "        text_embeddings = rearrange(text_embeddings, 'b l m -> b m l') # [batch_size, num_patches, 768]\n",
    "        return text_embeddings\n",
    "    \n",
    "    def get_ts_patch_embeddings(self, ts):\n",
    "        ts = rearrange(ts, 'b l m -> b m l')  #[batch_size, num_features, lookback]\n",
    "        ts = self.padding_patch_layer(ts) #[batch_size, num_features, lookback+stride]\n",
    "        ts = ts.unfold(dimension=-1, size=self.patch_size, step=self.stride) #[batch_size, num_features, num_patches, patch_size]\n",
    "        # Combines batch and feature dimensions\n",
    "        # num_features has be to 1: channel independence, otherwise, the batch size will be different from batch_size of text embeddings\n",
    "        ts = rearrange(ts, 'b m n p -> (b m) n p')  # [batch_size*num_features, num_patches, patch_size]\n",
    "        return ts\n",
    "        \n",
    "\n",
    "    def forward(self, ts, texts):\n",
    "        text_embeddings = self.get_patch_text_embeddings(texts) #([batch_size, num_patches, 768])\n",
    "        text_embeddings = self.relu(self.prompt_layer(text_embeddings)) #([batch_size, num_patches, 768])\n",
    "\n",
    "        # normalize time series for the whole lookback window todo\n",
    "        if self.revin:   \n",
    "            # ts: [batch_size, lookback, num_features]\n",
    "            ts = self.rev_in(ts, 'norm').to(self.device)\n",
    "        else:\n",
    "            # regular normalization todo\n",
    "            means = ts.mean(1, keepdim=True).detach() # detach from gpt4mts, necessary?\n",
    "            ts = ts - means\n",
    "            stdev = torch.sqrt(torch.var(ts, dim=1, keepdim=True, unbiased=False)+ 1e-5).detach() \n",
    "            ts /= stdev\n",
    "\n",
    "        ts_embeddings = self.get_ts_patch_embeddings(ts) #([batch_size, num_patches, 768])\n",
    "        ts_embeddings = self.in_layer(ts_embeddings) #([batch_size, num_patches, 768])\n",
    "        x_all = torch.cat((text_embeddings, ts_embeddings), dim=1) #([batch_size, num_patches*2, 768])\n",
    "\n",
    "        outputs = self.decoder(inputs_embeds=x_all).last_hidden_state # [batch_size, num_patches, 768]\n",
    "        # Take the last hidden state for prediction\n",
    "        outputs = outputs[:, -self.patch_num:, :]  # Return only the last b tokens\n",
    "        \n",
    "        # Reshape to [batch_size, num_patches * 768] for linear layer\n",
    "        a, b, c = outputs.shape\n",
    "        outputs = self.out_layer(outputs.reshape(a, b*c)) # [batch_size, predict]\n",
    "        outputs = outputs.unsqueeze(2) # [batch_size, predict, 1] 1 for one channel\n",
    "        \n",
    "\n",
    "        if self.revin:\n",
    "            outputs = self.rev_in(outputs, 'denorm').to(self.device)\n",
    "        else:\n",
    "            outputs = outputs * stdev\n",
    "            outputs = outputs + means\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8783],\n",
       "         [-0.7273]],\n",
       "\n",
       "        [[ 0.2631],\n",
       "         [ 0.6772]],\n",
       "\n",
       "        [[ 0.0835],\n",
       "         [ 0.3458]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test forward path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_config = Config(\n",
    "    lookback=4,\n",
    "    predict=2,\n",
    "    num_features=1,\n",
    "    patch_size=2,\n",
    "    stride=1,\n",
    "    model_input_dim=768,\n",
    "    model_hidden_dim=768,\n",
    "    finetunedecoder=True,\n",
    "    pretraingpt2=True,\n",
    "    revin=True\n",
    ")\n",
    "\n",
    "\n",
    "multimodal_gpt4mts(model_config, device)(\n",
    "    torch.randn(3, 4, 1), # [batch_size, lookback, num_features]\n",
    "    [[\"article1\", \"article2\", \"article3\", \"article4\"], \n",
    "     [\"article2\", \"article3\", \"article4\", \"article5\"], \n",
    "     [\"article3\", \"article4\", \"article5\", \"article6\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1236 rows in the filtered bitcoin dataframe\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_config = Config(\n",
    "    lookback=15,\n",
    "    predict=7,\n",
    "    num_features=1,\n",
    "    patch_size=8,\n",
    "    stride=4,\n",
    "    pretraingpt2=True,\n",
    "    model_input_dim=768,\n",
    "    model_hidden_dim=768,\n",
    "    finetunedecoder=True,\n",
    "    revin=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "patience = 3\n",
    "train_epochs = 10\n",
    "batch_size = 16\n",
    "weight_decay = 0\n",
    "model_save_path = \"multimodal_gpt4mts_iter0.pth\"\n",
    "\n",
    "# data parameters\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-04-22'\n",
    "ts_file_path = '../data/bitcoin/bitcoin_daily.csv'\n",
    "news_file_path = '../data/bitcoin/bitcoin_news.json'\n",
    "text_col = 'full_article'\n",
    "\n",
    "\n",
    "df_filtered = process_bitcoin_data(ts_file_path, news_file_path, start_date, end_date, text_col=text_col)\n",
    "train_data, val_data, test_data = split_series(df_filtered, model_config.lookback)\n",
    "\n",
    "# dataloader\n",
    "train_dataset = TimeSeriesDataset(train_data, model_config.lookback, model_config.predict, text_col=text_col)\n",
    "val_dataset = TimeSeriesDataset(val_data, model_config.lookback, model_config.predict, text_col=text_col)\n",
    "test_dataset = TimeSeriesDataset(test_data, model_config.lookback, model_config.predict, text_col=text_col)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "model = multimodal_gpt4mts(model_config, device)\n",
    "model.to(device)\n",
    "params = model.parameters()\n",
    "model_optim = torch.optim.Adam(params, lr=learning_rate, weight_decay=0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 0:\n",
      "    Train MSELoss: 659847.1875, Val MSELoss: 311653.0625, \n",
      "    Training RMSE: 812.3098, Val RMSE: 558.2590, \n",
      "    Training time: 8.47 minutes\n",
      "    \n",
      "New best model saved at epoch 0 with Val RMSE: 558.2590\n",
      "\n",
      "    Epoch 1:\n",
      "    Train MSELoss: 497146.6562, Val MSELoss: 340625.4688, \n",
      "    Training RMSE: 705.0863, Val RMSE: 583.6313, \n",
      "    Training time: 8.50 minutes\n",
      "    \n",
      "\n",
      "    Epoch 2:\n",
      "    Train MSELoss: 458341.4062, Val MSELoss: 259755.7188, \n",
      "    Training RMSE: 677.0092, Val RMSE: 509.6624, \n",
      "    Training time: 8.51 minutes\n",
      "    \n",
      "New best model saved at epoch 2 with Val RMSE: 509.6624\n",
      "\n",
      "    Epoch 3:\n",
      "    Train MSELoss: 430192.0625, Val MSELoss: 262738.7812, \n",
      "    Training RMSE: 655.8903, Val RMSE: 512.5805, \n",
      "    Training time: 8.48 minutes\n",
      "    \n",
      "\n",
      "    Epoch 4:\n",
      "    Train MSELoss: 411249.0000, Val MSELoss: 265007.7500, \n",
      "    Training RMSE: 641.2870, Val RMSE: 514.7891, \n",
      "    Training time: 8.52 minutes\n",
      "    \n",
      "\n",
      "    Epoch 5:\n",
      "    Train MSELoss: 406382.2188, Val MSELoss: 260391.4844, \n",
      "    Training RMSE: 637.4811, Val RMSE: 510.2857, \n",
      "    Training time: 8.50 minutes\n",
      "    \n",
      "\n",
      "    Epoch 6:\n",
      "    Train MSELoss: 408862.5938, Val MSELoss: 257538.7031, \n",
      "    Training RMSE: 639.4236, Val RMSE: 507.4827, \n",
      "    Training time: 8.53 minutes\n",
      "    \n",
      "New best model saved at epoch 6 with Val RMSE: 507.4827\n",
      "\n",
      "    Epoch 7:\n",
      "    Train MSELoss: 396188.6562, Val MSELoss: 284821.0000, \n",
      "    Training RMSE: 629.4352, Val RMSE: 533.6862, \n",
      "    Training time: 8.50 minutes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# model training \n",
    "\n",
    "best_val_mse = float('inf')\n",
    "best_epoch = -1\n",
    "for epoch in range(train_epochs):\n",
    "    train_loss = []\n",
    "    all_train_preds = []\n",
    "    all_train_targets = []\n",
    "    epoch_time = time.time()\n",
    "    for i, train_loader_item in enumerate(train_loader):\n",
    "        texts = train_loader_item['text']\n",
    "        ts_values = train_loader_item['value'].float().to(device)\n",
    "        targets = train_loader_item['target'].float().to(device)\n",
    "        model_optim.zero_grad()\n",
    "\n",
    "        outputs = model(ts_values, texts)\n",
    "        all_train_preds.append(outputs.detach().cpu().numpy())\n",
    "        all_train_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "    all_train_preds = np.concatenate(all_train_preds, axis=0)\n",
    "    all_train_targets = np.concatenate(all_train_targets, axis=0)\n",
    "    train_mse = np.mean((all_train_preds - all_train_targets) ** 2)\n",
    "    # avg_train_loss = np.average(train_loss)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    all_val_preds = []\n",
    "    all_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for i, val_loader_item in enumerate(val_loader):\n",
    "            texts = val_loader_item['text']\n",
    "            ts_values = val_loader_item['value'].float().to(device)\n",
    "            targets = val_loader_item['target'].float().to(device)\n",
    "            outputs = model(ts_values, texts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_val_preds.append(outputs.detach().cpu().numpy())\n",
    "            all_val_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "    all_val_preds = np.concatenate(all_val_preds, axis=0)\n",
    "    all_val_targets = np.concatenate(all_val_targets, axis=0)\n",
    "    val_mse = np.mean((all_val_preds - all_val_targets) ** 2)\n",
    "\n",
    "    # avg_val_loss = np.average(val_loss)\n",
    "    model.train()\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Epoch {epoch}:\n",
    "    Train MSELoss: {train_mse:.4f}, Val MSELoss: {val_mse:.4f}, \n",
    "    Training RMSE: {np.sqrt(train_mse):.4f}, Val RMSE: {np.sqrt(val_mse):.4f}, \n",
    "    Training time: {(time.time() - epoch_time) / 60:.2f} minutes\n",
    "    \"\"\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved at epoch {epoch} with Val RMSE: {np.sqrt(val_mse):.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch > 0 and val_loss[-1] > val_loss[-2]:\n",
    "        consec_increase += 1\n",
    "    else:\n",
    "        consec_increase = 0\n",
    "\n",
    "    if consec_increase == patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1} as val loss has been increasing for {patience} epochs \\\n",
    "              for {consec_increase} epochs\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model Val MSELoss: 238667.0625, \n",
      "    Last Point Baseline MSELoss: 263000.5938, \n",
      "    Average Window Baseline MSELoss: 727225.7500, \n",
      "    Model RMSE: 488.5356, \n",
      "    Last Point Baseline RMSE: 512.8358, \n",
      "    Average Window Baseline RMSE: 852.7753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# naive baseline\n",
    "all_last_point_forecasts = []\n",
    "all_avg_window_forecasts = []\n",
    "all_targets = []\n",
    "\n",
    "for i, val_loader_item in enumerate(val_loader):\n",
    "    ts_values = val_loader_item['value'].float()\n",
    "    targets = val_loader_item['target'].float()\n",
    "    \n",
    "    # Last point baseline\n",
    "    last_values = ts_values[:, -1:, :]\n",
    "    last_point_forecast = last_values.repeat(1, model_config.predict, 1)\n",
    "    all_last_point_forecasts.append(last_point_forecast)\n",
    "    \n",
    "    # Average of lookback window baseline\n",
    "    avg_values = ts_values.mean(dim=1, keepdim=True)  # shape: [batch, 1, features]\n",
    "    avg_forecast = avg_values.repeat(1, model_config.predict, 1)\n",
    "    all_avg_window_forecasts.append(avg_forecast)\n",
    "    \n",
    "    # Collect targets\n",
    "    all_targets.append(targets)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_last_point_forecasts = torch.cat(all_last_point_forecasts, dim=0)\n",
    "all_avg_window_forecasts = torch.cat(all_avg_window_forecasts, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "# Compute MSE and RMSE\n",
    "last_point_mse = torch.mean((all_last_point_forecasts - all_targets) ** 2).item()\n",
    "avg_window_mse = torch.mean((all_avg_window_forecasts - all_targets) ** 2).item()\n",
    "\n",
    "print(f\"\"\"\n",
    "    Model Val MSELoss: {best_val_mse:.4f}, \n",
    "    Last Point Baseline MSELoss: {last_point_mse:.4f}, \n",
    "    Average Window Baseline MSELoss: {avg_window_mse:.4f}, \n",
    "    Model RMSE: {np.sqrt(best_val_mse):.4f}, \n",
    "    Last Point Baseline RMSE: {np.sqrt(last_point_mse):.4f}, \n",
    "    Average Window Baseline RMSE: {np.sqrt(avg_window_mse):.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results log \n",
    "# model_config = Config(\n",
    "#     lookback=15,\n",
    "#     predict=7,\n",
    "#     num_features=1,\n",
    "#     patch_size=8,\n",
    "#     stride=4,\n",
    "#     pretraingpt2=True,\n",
    "#     model_input_dim=768,\n",
    "#     model_hidden_dim=768,\n",
    "#     finetunedecoder=True,\n",
    "#     revin=True\n",
    "# )\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # training parameters\n",
    "# learning_rate = 0.001\n",
    "# patience = 3\n",
    "# train_epochs = 10\n",
    "# batch_size = 16\n",
    "# weight_decay = 0\n",
    "# model_save_path = \"multimodal_gpt4mts_iter0.pth\"\n",
    "\n",
    "# # data parameters\n",
    "# start_date = '2018-01-01'\n",
    "# end_date = '2022-04-22'\n",
    "# ts_file_path = '../data/bitcoin/bitcoin_daily.csv'\n",
    "\n",
    "    # news_file_path = '../data/bitcoin/bitcoin_news_with_summaries.json'\n",
    "    # Model RMSE: 503.0874, \n",
    "    # Last Point Baseline RMSE: 512.8358, \n",
    "    # Average Window Baseline RMSE: 852.7753\n",
    "\n",
    "\n",
    "    # news_file_path = '../data/bitcoin/bitcoinprice_news_selected_one_per_day.json'\n",
    "    #     Model RMSE: 488.5356, \n",
    "    #     Last Point Baseline RMSE: 512.8358, \n",
    "    #     Average Window Baseline RMSE: 852.7753\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
