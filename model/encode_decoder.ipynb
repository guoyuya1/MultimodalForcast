{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyan/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Adjust the path as needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time\n",
    "from einops import rearrange\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from MultimodalForcast.data_loader.data_loader import process_bitcoin_data, split_series, TimeSeriesDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 91 rows in the filtered bitcoin dataframe\n",
      "Batch shapes:\n",
      "Texts: 8 sequences, each with 10 articles\n",
      "Values shape: torch.Size([8, 10, 1])\n",
      "Targets shape: torch.Size([8, 10, 1])\n",
      "\n",
      "First sequence:\n",
      "Values: tensor([[ 9233.],\n",
      "        [ 9421.],\n",
      "        [ 9706.],\n",
      "        [ 9798.],\n",
      "        [ 9823.],\n",
      "        [10083.],\n",
      "        [ 9932.],\n",
      "        [ 9968.],\n",
      "        [10331.],\n",
      "        [10289.]])\n",
      "Targets: tensor([[10272.],\n",
      "        [10152.],\n",
      "        [ 9916.],\n",
      "        [ 9723.],\n",
      "        [ 9840.],\n",
      "        [10091.],\n",
      "        [ 9612.],\n",
      "        [ 9688.],\n",
      "        [ 9666.],\n",
      "        [ 9850.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyan/MultimodalForcast/model/../../MultimodalForcast/data_loader/data_loader.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return x_text, torch.tensor(x_value, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2020-03-31'\n",
    "ts_file_path = '../data/bitcoin/bitcoin_daily.csv'\n",
    "news_file_path = '../data/bitcoin/bitcoin_news.json'\n",
    "lookback = 10\n",
    "predict = 10\n",
    "\n",
    "df_filtered = process_bitcoin_data(ts_file_path, news_file_path, start_date, end_date)\n",
    "train_data, val_data, test_data = split_series(df_filtered, lookback)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, lookback, predict)\n",
    "val_dataset = TimeSeriesDataset(val_data, lookback, predict)\n",
    "test_dataset = TimeSeriesDataset(test_data, lookback, predict)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "texts = batch['text']      # List of article sequences\n",
    "values = batch['value']    # Tensor of value sequences\n",
    "targets = batch['target']  # Tensor of target values\n",
    "\n",
    "# Print shapes and sample content\n",
    "print(\"Batch shapes:\")\n",
    "print(f\"Texts: {len(texts)} sequences, each with {len(texts[0])} articles\")\n",
    "print(f\"Values shape: {values.shape}\")  # Should be [batch_size, lookback_window]\n",
    "print(f\"Targets shape: {targets.shape}\")  # Should be [batch_size, prediction_window]\n",
    "\n",
    "# Print first sequence's content\n",
    "print(\"\\nFirst sequence:\")\n",
    "# print(\"Texts:\", texts[0])\n",
    "print(\"Values:\", values[0])\n",
    "print(\"Targets:\", targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  draft of pretrained bert text encoder\n",
    "# # pretrained bert text encoder - draft\n",
    "# # bert-base-uncased: 768\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# # tocuda\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# texts = df_filtered['full_article'].values.tolist()\n",
    "# # Removing special characters (keeping only alphanumeric and spaces)\n",
    "# #If text is not a string, then NA\n",
    "# texts = [\n",
    "#     re.sub(r'[^a-zA-Z0-9 ]+', \"\", str(text)) \n",
    "#     if pd.notna(text) and isinstance(text, (str, float, int)) \n",
    "#     else \"NA\" \n",
    "#     for text in texts\n",
    "# ]\n",
    "\n",
    "\n",
    "# batch_size = 8 # number of text sequences in one batch\n",
    "# all_embeddings = []\n",
    "\n",
    "# # Process each batch\n",
    "# for i in range(0, len(texts), batch_size):\n",
    "#     batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "#     # Tokenize the batch\n",
    "#     # padding=True, truncation=True, max_length=512\n",
    "#     # tocuda\n",
    "#     # inputs: [batch_size, sequence_length]\n",
    "#     inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "#     # Get the embeddings\n",
    "#     with torch.no_grad():\n",
    "#         # outputs: [batch_size, sequence_length, bert_output_hidden_size]\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     # [CLS]\n",
    "#     # batch_embeddings: [batch_size, bert_output_hidden_size]\n",
    "#     batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#     all_embeddings.append(batch_embeddings)\n",
    "\n",
    "# # Combine all batches    \n",
    "# if all_embeddings:\n",
    "#     # all_embeddings: [total_number_of_texts, bert_output_hidden_size]\n",
    "#     all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# print(all_embeddings.shape)\n",
    "\n",
    "# # text encoder\n",
    "# # input: list of list\n",
    "# # output: [batch size, look_back, bert_hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrained_Bert_Encoder(nn.Module):\n",
    "    def __init__(self, device, finetune=False):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        \n",
    "        self.device = device\n",
    "        self.bert = self.bert.to(device)\n",
    "        # self.linear = self.linear.to(device)\n",
    "        \n",
    "        # Freeze BERT parameters\n",
    "        if finetune == False:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        # texts is a list of strings\n",
    "        texts = [\n",
    "        re.sub(r'[^a-zA-Z0-9 ]+', \"\", str(text)) \n",
    "        if pd.notna(text) and isinstance(text, (str, float, int)) \n",
    "        else \"NA\" \n",
    "        for text in texts\n",
    "    ]\n",
    "        return texts\n",
    "            \n",
    "    def forward(self, texts):\n",
    "        # texts is a list of lists: [batch_size, lookback]\n",
    "        \n",
    "        # Flatten the list of lists into a single list\n",
    "        flat_texts = [article for sample_texts in texts for article in sample_texts]\n",
    "        \n",
    "        # Preprocess all texts\n",
    "        flat_texts = self.preprocess_text(flat_texts)\n",
    "        \n",
    "        # Tokenize and get embeddings\n",
    "        inputs = self.tokenizer(flat_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.bert.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(**inputs)\n",
    "        \n",
    "        # Get [CLS] token embeddings\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [total_articles, 768]\n",
    "\n",
    "        \n",
    "        # Reshape back to [batch_size, lookback, bert_output_dim]\n",
    "        batch_size = len(texts)\n",
    "        lookback = len(texts[0])  #lookback\n",
    "        reshaped_output = cls_embeddings.reshape(batch_size, lookback, -1)\n",
    "        \n",
    "        return reshaped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test encoder\n",
    "encoder = Pretrained_Bert_Encoder(device=\"cpu\")\n",
    "encoder.forward([[\"article1\", \"article2\"], [\"article2\", \"article3\"], [\"article3\", \"article4\"]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RevIN (Reversible Instance Normalization)\n",
    "\n",
    "class RevIn(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: Number of input features\n",
    "        :param eps: Stability add-on value\n",
    "        :param affine: If True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIn, self).__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        # x is [batch_size, seq_len, num_features]:\n",
    "        # range(1, x.ndim - 1) = range(1, 2) = (1,)\n",
    "        # along along sequence dimension\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last.to(x.device)\n",
    "        else:\n",
    "            x = x - self.mean.to(x.device)\n",
    "        x = x / self.stdev.to(x.device)\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight.to(x.device)\n",
    "            x = x + self.affine_bias.to(x.device)\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias.to(x.device)\n",
    "            x = x / (self.affine_weight.to(x.device) + self.eps * self.eps)\n",
    "        x = x * self.stdev.to(x.device)\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last.to(x.device)\n",
    "        else:\n",
    "            x = x + self.mean.to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # detach() example\n",
    "# w = nn.Parameter(torch.tensor([2.0]), requires_grad=True)\n",
    "\n",
    "# # Case 1: No detach\n",
    "# result = 2*w + w  # = 3w\n",
    "# loss = some_function(result)\n",
    "# loss.backward()\n",
    "# # Gradient flows: loss -> result -> w\n",
    "# # w.grad will include factor of 3 (both paths contribute)\n",
    "# # If loss gradient is 1, w.grad would be 3\n",
    "\n",
    "# # Case 2: Partial detach\n",
    "# result = (2*w).detach() + w  # = 2(constant) + w\n",
    "# loss = some_function(result)\n",
    "# loss.backward()\n",
    "# # Gradient flows only through the 'w' term\n",
    "# # 2*w is treated as constant\n",
    "# # If loss gradient is 1, w.grad would be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multimodal_gpt4mts(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patching parameters\n",
    "        self.patch_size = config.patch_size\n",
    "        self.stride = config.stride\n",
    "        self.patch_num = (config.lookback - self.patch_size) // self.stride + 2\n",
    "        self.revin = config.revin\n",
    "        self.device = device\n",
    "\n",
    "        # Original sequence: [1, 2, 3, 4, 5]\n",
    "        # With (0, 2) padding:\n",
    "        # - Add 0 elements at start\n",
    "        # - Add 2 elements at end by replicating last value\n",
    "        # Result: [1, 2, 3, 4, 5, 5, 5]\n",
    "\n",
    "        # If we used (2, 2):\n",
    "        # - Add 2 elements at start by replicating first value\n",
    "        # - Add 2 elements at end by replicating last value\n",
    "        # Result: [1, 1, 1, 2, 3, 4, 5, 5, 5]\n",
    "        self.padding_patch_layer = nn.ReplicationPad1d((0, config.stride))\n",
    "      \n",
    "\n",
    "        # encoder decoder\n",
    "        \n",
    "        self.in_layer = nn.Linear(config.patch_size, config.model_hidden_dim)\n",
    "        self.prompt_layer = nn.Linear(config.model_hidden_dim, config.model_hidden_dim)\n",
    "        self.out_layer = nn.Linear(config.model_hidden_dim * (self.patch_num), config.predict)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rev_in = RevIn(config.num_features)\n",
    "        \n",
    "        # text encoder, currently frozen\n",
    "        self.text_encoder = Pretrained_Bert_Encoder(finetune=False, device=device)\n",
    "\n",
    "        if config.pretraingpt2:\n",
    "            self.decoder = GPT2Model.from_pretrained('gpt2')\n",
    "        else:\n",
    "            self.decoder = GPT2Model(GPT2Config())\n",
    "\n",
    "\n",
    "        if config.pretraingpt2 and config.finetunedecoder:\n",
    "            # Only fine-tunes layer normalization and positional embeddings\n",
    "            # Layer norms help adapt to new data distributions\n",
    "            # layer norm operates on the last dimension - 766 for each patch\n",
    "\n",
    "            for i, (name, param) in enumerate(self.decoder.named_parameters()):\n",
    "                if 'ln' in name or 'wpe' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "\n",
    "        for layer in (self.decoder, self.in_layer, self.out_layer, self.prompt_layer):\n",
    "            layer.to(device=device)\n",
    "\n",
    "    def get_patch_text_embeddings(self, texts):\n",
    "        text_embeddings = self.text_encoder(texts)\n",
    "        text_embeddings = rearrange(text_embeddings, 'b l m -> b m l') # [batch_size, 768, lookback]\n",
    "        text_embeddings = self.padding_patch_layer(text_embeddings) # [batch_size, 768, lookback + stride]\n",
    "\n",
    "        # patch along the last dimension\n",
    "        text_embeddings = text_embeddings.unfold(dimension=-1, size=self.patch_size, step=self.stride) #[batch_size, 768, num_patches, patch_size]\n",
    "        # average text embedding of each patch\n",
    "        text_embeddings = text_embeddings.mean(dim=-1).squeeze() # [batch_size, 768, num_patches]\n",
    "        text_embeddings = rearrange(text_embeddings, 'b l m -> b m l') # [batch_size, num_patches, 768]\n",
    "        return text_embeddings\n",
    "    \n",
    "    def get_ts_patch_embeddings(self, ts):\n",
    "        ts = rearrange(ts, 'b l m -> b m l')  #[batch_size, num_features, lookback]\n",
    "        ts = self.padding_patch_layer(ts) #[batch_size, num_features, lookback+stride]\n",
    "        ts = ts.unfold(dimension=-1, size=self.patch_size, step=self.stride) #[batch_size, num_features, num_patches, patch_size]\n",
    "        # Combines batch and feature dimensions\n",
    "        # num_features has be to 1: channel independence, otherwise, the batch size will be different from batch_size of text embeddings\n",
    "        ts = rearrange(ts, 'b m n p -> (b m) n p')  # [batch_size*num_features, num_patches, patch_size]\n",
    "        return ts\n",
    "        \n",
    "\n",
    "    def forward(self, ts, texts):\n",
    "        text_embeddings = self.get_patch_text_embeddings(texts) #([batch_size, num_patches, 768])\n",
    "        text_embeddings = self.relu(self.prompt_layer(text_embeddings)) #([batch_size, num_patches, 768])\n",
    "\n",
    "        # normalize time series for the whole lookback window todo\n",
    "        if self.revin:   \n",
    "            # ts: [batch_size, lookback, num_features]\n",
    "            ts = self.rev_in(ts, 'norm').to(self.device)\n",
    "        else:\n",
    "            # regular normalization todo\n",
    "            means = ts.mean(1, keepdim=True).detach() # detach from gpt4mts, necessary?\n",
    "            ts = ts - means\n",
    "            stdev = torch.sqrt(torch.var(ts, dim=1, keepdim=True, unbiased=False)+ 1e-5).detach() \n",
    "            ts /= stdev\n",
    "\n",
    "        ts_embeddings = self.get_ts_patch_embeddings(ts) #([batch_size, num_patches, 768])\n",
    "        ts_embeddings = self.in_layer(ts_embeddings) #([batch_size, num_patches, 768])\n",
    "        x_all = torch.cat((text_embeddings, ts_embeddings), dim=1) #([batch_size, num_patches*2, 768])\n",
    "\n",
    "        outputs = self.decoder(inputs_embeds=x_all).last_hidden_state # [batch_size, num_patches, 768]\n",
    "        # Take the last hidden state for prediction\n",
    "        outputs = outputs[:, -self.patch_num:, :]  # Return only the last b tokens\n",
    "        \n",
    "        # Reshape to [batch_size, num_patches * 768] for linear layer\n",
    "        a, b, c = outputs.shape\n",
    "        outputs = self.out_layer(outputs.reshape(a, b*c)) # [batch_size, predict]\n",
    "        outputs = outputs.unsqueeze(2) # [batch_size, predict, 1] 1 for one channel\n",
    "        \n",
    "\n",
    "        if self.revin:\n",
    "            outputs = self.rev_in(outputs, 'denorm').to(self.device)\n",
    "        else:\n",
    "            outputs = outputs * stdev\n",
    "            outputs = outputs + means\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0322],\n",
       "         [-0.3327]],\n",
       "\n",
       "        [[-0.0573],\n",
       "         [-0.3371]],\n",
       "\n",
       "        [[ 0.2202],\n",
       "         [ 0.1671]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test forward path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_config = Config(\n",
    "    lookback=4,\n",
    "    predict=2,\n",
    "    num_features=1,\n",
    "    patch_size=2,\n",
    "    stride=1,\n",
    "    model_input_dim=768,\n",
    "    model_hidden_dim=768,\n",
    "    finetunedecoder=True,\n",
    "    pretraingpt2=True,\n",
    "    revin=True\n",
    ")\n",
    "\n",
    "\n",
    "multimodal_gpt4mts(model_config, device)(\n",
    "    torch.randn(3, 4, 1), # [batch_size, lookback, num_features]\n",
    "    [[\"article1\", \"article2\", \"article3\", \"article4\"], \n",
    "     [\"article2\", \"article3\", \"article4\", \"article5\"], \n",
    "     [\"article3\", \"article4\", \"article5\", \"article6\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1236 rows in the filtered bitcoin dataframe\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_config = Config(\n",
    "    lookback=15,\n",
    "    predict=7,\n",
    "    num_features=1,\n",
    "    patch_size=8,\n",
    "    stride=4,\n",
    "    pretraingpt2=True,\n",
    "    model_input_dim=768,\n",
    "    model_hidden_dim=768,\n",
    "    finetunedecoder=True,\n",
    "    revin=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "patience = 3\n",
    "train_epochs = 10\n",
    "batch_size = 16\n",
    "weight_decay = 0\n",
    "model_save_path = \"multimodal_gpt4mts_iter0.pth\"\n",
    "\n",
    "# data parameters\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-04-22'\n",
    "ts_file_path = '../data/bitcoin/bitcoin_daily.csv'\n",
    "news_file_path = '../data/bitcoin/bitcoin_news_with_summaries.json'\n",
    "\n",
    "\n",
    "df_filtered = process_bitcoin_data(ts_file_path, news_file_path, start_date, end_date, text_col='summary')\n",
    "train_data, val_data, test_data = split_series(df_filtered, model_config.lookback)\n",
    "\n",
    "# dataloader\n",
    "train_dataset = TimeSeriesDataset(train_data, model_config.lookback, model_config.predict, text_col='summary')\n",
    "val_dataset = TimeSeriesDataset(val_data, model_config.lookback, model_config.predict, text_col='summary')\n",
    "test_dataset = TimeSeriesDataset(test_data, model_config.lookback, model_config.predict, text_col='summary')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "model = multimodal_gpt4mts(model_config, device)\n",
    "model.to(device)\n",
    "params = model.parameters()\n",
    "model_optim = torch.optim.Adam(params, lr=learning_rate, weight_decay=0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 0:\n",
      "    Train MSELoss: 659531.0625, Val MSELoss: 292673.7500, \n",
      "    Training RMSE: 812.1152, Val RMSE: 540.9933, \n",
      "    Training time: 2.39 minutes\n",
      "    \n",
      "New best model saved at epoch 0 with Val RMSE: 540.9933\n",
      "\n",
      "    Epoch 1:\n",
      "    Train MSELoss: 495931.2812, Val MSELoss: 296300.2188, \n",
      "    Training RMSE: 704.2239, Val RMSE: 544.3347, \n",
      "    Training time: 2.46 minutes\n",
      "    \n",
      "\n",
      "    Epoch 2:\n",
      "    Train MSELoss: 432079.3750, Val MSELoss: 266626.8125, \n",
      "    Training RMSE: 657.3275, Val RMSE: 516.3592, \n",
      "    Training time: 2.46 minutes\n",
      "    \n",
      "New best model saved at epoch 2 with Val RMSE: 516.3592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m ts_values \u001b[38;5;241m=\u001b[39m val_loader_item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m targets \u001b[38;5;241m=\u001b[39m val_loader_item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     43\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mmultimodal_gpt4mts.forward\u001b[0;34m(self, ts, texts)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ts, texts):\n\u001b[0;32m---> 80\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_patch_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#([batch_size, num_patches, 768])\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_layer(text_embeddings)) \u001b[38;5;66;03m#([batch_size, num_patches, 768])\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# normalize time series for the whole lookback window todo\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m, in \u001b[0;36mmultimodal_gpt4mts.get_patch_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_patch_text_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[0;32m---> 58\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m rearrange(text_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb l m -> b m l\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# [batch_size, 768, lookback]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_patch_layer(text_embeddings) \u001b[38;5;66;03m# [batch_size, 768, lookback + stride]\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mPretrained_Bert_Encoder.forward\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     34\u001b[0m flat_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_text(flat_texts)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Tokenize and get embeddings\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2887\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2887\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2975\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2972\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2973\u001b[0m         )\n\u001b[1;32m   2974\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2998\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2999\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3018\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3177\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3167\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3168\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3169\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3170\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3175\u001b[0m )\n\u001b[0;32m-> 3177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3179\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:887\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    885\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 887\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    889\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:854\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 854\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:653\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    648\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[1;32m    651\u001b[0m     ]\n\u001b[1;32m    652\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 653\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[1;32m    656\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/lib/python3.10/re.py:209\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:653\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.<lambda>\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m    647\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    648\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[1;32m    651\u001b[0m     ]\n\u001b[1;32m    652\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 653\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[1;32m    656\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model training \n",
    "\n",
    "best_val_mse = float('inf')\n",
    "best_epoch = -1\n",
    "for epoch in range(train_epochs):\n",
    "    train_loss = []\n",
    "    all_train_preds = []\n",
    "    all_train_targets = []\n",
    "    epoch_time = time.time()\n",
    "    for i, train_loader_item in enumerate(train_loader):\n",
    "        texts = train_loader_item['text']\n",
    "        ts_values = train_loader_item['value'].float().to(device)\n",
    "        targets = train_loader_item['target'].float().to(device)\n",
    "        model_optim.zero_grad()\n",
    "\n",
    "        outputs = model(ts_values, texts)\n",
    "        all_train_preds.append(outputs.detach().cpu().numpy())\n",
    "        all_train_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "    all_train_preds = np.concatenate(all_train_preds, axis=0)\n",
    "    all_train_targets = np.concatenate(all_train_targets, axis=0)\n",
    "    train_mse = np.mean((all_train_preds - all_train_targets) ** 2)\n",
    "    # avg_train_loss = np.average(train_loss)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    all_val_preds = []\n",
    "    all_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for i, val_loader_item in enumerate(val_loader):\n",
    "            texts = val_loader_item['text']\n",
    "            ts_values = val_loader_item['value'].float().to(device)\n",
    "            targets = val_loader_item['target'].float().to(device)\n",
    "            outputs = model(ts_values, texts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_val_preds.append(outputs.detach().cpu().numpy())\n",
    "            all_val_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "    all_val_preds = np.concatenate(all_val_preds, axis=0)\n",
    "    all_val_targets = np.concatenate(all_val_targets, axis=0)\n",
    "    val_mse = np.mean((all_val_preds - all_val_targets) ** 2)\n",
    "\n",
    "    # avg_val_loss = np.average(val_loss)\n",
    "    model.train()\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Epoch {epoch}:\n",
    "    Train MSELoss: {train_mse:.4f}, Val MSELoss: {val_mse:.4f}, \n",
    "    Training RMSE: {np.sqrt(train_mse):.4f}, Val RMSE: {np.sqrt(val_mse):.4f}, \n",
    "    Training time: {(time.time() - epoch_time) / 60:.2f} minutes\n",
    "    \"\"\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved at epoch {epoch} with Val RMSE: {np.sqrt(val_mse):.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch > 0 and val_loss[-1] > val_loss[-2]:\n",
    "        consec_increase += 1\n",
    "    else:\n",
    "        consec_increase = 0\n",
    "\n",
    "    if consec_increase == patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1} as val loss has been increasing for {patience} epochs \\\n",
    "              for {consec_increase} epochs\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model Val MSELoss: 245810.2656, \n",
      "    Last Point Baseline MSELoss: 263000.5938, \n",
      "    Average Window Baseline MSELoss: 727225.7500, \n",
      "    Model RMSE: 495.7926, \n",
      "    Last Point Baseline RMSE: 512.8358, \n",
      "    Average Window Baseline RMSE: 852.7753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# naive baseline\n",
    "all_last_point_forecasts = []\n",
    "all_avg_window_forecasts = []\n",
    "all_targets = []\n",
    "\n",
    "for i, val_loader_item in enumerate(val_loader):\n",
    "    ts_values = val_loader_item['value'].float()\n",
    "    targets = val_loader_item['target'].float()\n",
    "    \n",
    "    # Last point baseline\n",
    "    last_values = ts_values[:, -1:, :]\n",
    "    last_point_forecast = last_values.repeat(1, model_config.predict, 1)\n",
    "    all_last_point_forecasts.append(last_point_forecast)\n",
    "    \n",
    "    # Average of lookback window baseline\n",
    "    avg_values = ts_values.mean(dim=1, keepdim=True)  # shape: [batch, 1, features]\n",
    "    avg_forecast = avg_values.repeat(1, model_config.predict, 1)\n",
    "    all_avg_window_forecasts.append(avg_forecast)\n",
    "    \n",
    "    # Collect targets\n",
    "    all_targets.append(targets)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_last_point_forecasts = torch.cat(all_last_point_forecasts, dim=0)\n",
    "all_avg_window_forecasts = torch.cat(all_avg_window_forecasts, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "# Compute MSE and RMSE\n",
    "last_point_mse = torch.mean((all_last_point_forecasts - all_targets) ** 2).item()\n",
    "avg_window_mse = torch.mean((all_avg_window_forecasts - all_targets) ** 2).item()\n",
    "\n",
    "print(f\"\"\"\n",
    "    Model Val MSELoss: {best_val_mse:.4f}, \n",
    "    Last Point Baseline MSELoss: {last_point_mse:.4f}, \n",
    "    Average Window Baseline MSELoss: {avg_window_mse:.4f}, \n",
    "    Model RMSE: {np.sqrt(best_val_mse):.4f}, \n",
    "    Last Point Baseline RMSE: {np.sqrt(last_point_mse):.4f}, \n",
    "    Average Window Baseline RMSE: {np.sqrt(avg_window_mse):.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
